---
title: "Ch_8_Explporer_R"
author: "Yelena Gomez"
date: "`r format(Sys.Date())`"
output: html_document
---

# Quelques outils en √©cologie math√©matique avec R

Il arrive souvent ques les donn√©es brutes ne soient pas exprim√©es de mani√®re appropri√©e ou optimale pour l‚Äôanalyse statistique ou la mod√©lisation. Vous devrez alors effectuer un pr√©traitement sur ces donn√©es.

Un pr√©traitement peut consister simplement en une transformation logarithmique ou exponentielle. Nous verrons les transformations les plus communes comme la standardisation, la mise √† l‚Äô√©chelle sur une √©tendue et la normalisation. Puis nous verrons comment ces op√©rations de pr√©traitement sont offertes dans le module `recipes`.

`recipes` n‚Äôest pas en mesure d‚Äôeffectuer toutes les transformations imaginables. Pour des op√©rations plus sp√©cialis√©es, si vos donn√©es forment une partie d‚Äôun tout (exprim√©es en pourcentages ou fractions), vous devriez probablement utiliser un pr√©traitement gr√¢ce aux outils de l‚Äôanalyse compositionnelle. Avant de les aborder, nous allons traiter des transformations de base.

## Standardisation

La standardisation consiste √† centrer vos donn√©es √† une moyenne de 0 et √† les √©chelonner √† une variance de 1.

Ce pr√©traitement des donn√©es peut s‚Äôav√©r√©r utile lorsque la mod√©lisation tient compte de l‚Äô√©chelle de vos mesures (par exemple, les param√®tres de r√©gression vus au chapitre 6 ou les distances que nous verrons au chapitre 9). En effet, les pentes d‚Äôune r√©gression lin√©aire multiple ne pourront √™tre compar√©es entre elles que si elles sont une m√™me √©chelle. Par exemple, on veut mod√©liser la consommation en miles au gallon (mpg) de voitures en fonction de leur puissance (hp), le temps en secondes pour parcourir un quart de mile (qsec) et le nombre de cylindre.


```{r}
data("mtcars")
modl <- lm(mpg ~ hp + qsec + cyl, mtcars)
summary(modl)
```
Les pentes signifient que la distance parcourue par gallon d‚Äôessence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L‚Äôinterpr√©tation est conviviale √† cette √©chelle. Mais lequel de ces effets est le plus important? L t value indique que ce seraient les cylindres. Mais pour juger l‚Äôimportance en terme de pente, il vaudrait mieux standardiser.

```{r}
library("tidyverse")
```


```{r}
standardise <- function(x) 
  (x-mean(x))/sd(x)
mtcars_sc <- mtcars %>%
  mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE)
modl_sc <- lm(mpg ~ hp + qsec + cyl, mtcars_sc)
summary(modl_sc)
```
Les valeurs des pentes ne peuvent plus √™tre interpr√©t√©es directement, mais peuvent maintenant √™tre compar√©es entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile.

Les algorithmes bas√©s sur des distances auront, de m√™me, avantage √† √™tre standardis√©s.

## √Ä l‚Äô√©chelle de la plage

Si vous d√©sirez pr√©server le z√©ro dans le cas de donn√©es positives ou plus g√©n√©ralement vous voulez que vos donn√©es pr√©trait√©es soient positives, vous pouvez les transformer √† l‚Äô√©chelle de la plage, c‚Äôest-√†-dire les forcer √† s‚Äô√©taler de 0 √† 1:

Cette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transform√© les valeurs aberrantes seront toutefois plus difficiles √† d√©tecter.

```{r}
range_01 <- function(x) (x-min(x))/(max(x) - min(x))
mtcars %>%
  mutate_if(is.numeric, range_01) %>% # en fait, toutes les colonnes sont num√©riques, alors mutate_all aurait pu √™tre utilis√© au lieu de mutate_if
  sample_n(4)
```

## Normaliser

Le terme *normaliser* est associer √† des op√©rations diff√©rentes dans la litt√©rature. Nous prendrons la nomenclature de scikit-learn, pour qui la normalisation consiste √† faire en sorte que la longueur du vecteur (sa norme, d‚Äôo√π normaliser) soit unitaire. Cette op√©ration est le plus souvent utilis√©e par observation (ligne), non pas par variable (colonne). Il existe plusieurs mani√®res de mesures la distance d‚Äôun vecteur, mais la plus commune est la distance euclidienne.

```{r}
library("pls")
```


```{r}
data("gasoline")
spectro <- gasoline$NIR %>% unclass() %>% as_tibble()

normalise <- function(x) x/sqrt(sum(x^2))
spectro_norm <- spectro %>% 
  rowwise() %>% # diff√©rentes approches possibles pour les op√©rations sur les lignes
  normalise()
spectro_norm[1:4, 1:4]
```

## Le module recipes

Nous avons vu comment standardiser avec notre propre fonction. Certaines personnes pr√©f√®rent utiliser la fonction `scale()`. Mais une nouvelle approche est en train de s‚Äôinstaller, avec le module `recipes`, un module de l‚Äôombrelle `tidymodels`, un m√©ta module en d√©veloppement visant √† faire de R un outil de mod√©lisation plus convivial.

`recipes` fonctionne en mode *tidyverse*, c‚Äôest-√†-dire en suites d‚Äôop√©rations. De nombreuses fonctions sont offertes, dont des fonctions d‚Äôimputation, que nous verrons au chapitre 10. Nous couvrirons ici la standardisation et la mise √† l‚Äô√©chelle, juste pour l‚Äôap√©ro üç≥.

Le module ne s‚Äôappelle pas recette pour rien. Il fonctionne en trois √©tapes:

1. Monter la liste des ingr√©dients: sp√©cifier ce qu‚Äôil faut faire
1. M√©langer les ingr√©dients: transformer tout ce qu‚Äôil faut faire en une proc√©dure
1. Cuire les ingr√©dients: appliquer la proc√©dure √† un tableau.

Voici une petite application sur le tableau `lasrosas.corn`.

```{r, warning=FALSE, message = FALSE, error = FALSE}
library("tidymodels")
```


```{r}
data(lasrosas.corn, package = "agridat")
lasrosas.corn %>% 
  head()
```

Disons que pour mon mod√®le statistique, ma variable de sortie est le rendement (yield), que je d√©sire lier √† la dose d‚Äôazote (nitro), √† un indicateur de la teneur en mati√®re organique du sol (bv) et √† la topographie (topo).

Mais pour rendre le mod√®le pr√©dictif (et non pas seulement descriptif), je dois l‚Äô√©valuer sur des donn√©es qui n‚Äôont pas servies √† lisser le mod√®le (nous verrons en plus de d√©tails √ßa au chapitre 12). Je vais donc s√©parer mon tableau au hasard en un tableau d‚Äôentra√Ænement comprenant 70% des observations et un autre pour tester le mod√®le comprenant le 30% restant.

```{r}
train_test_split <- lasrosas.corn %>% 
  select(yield, nitro, bv, topo) %>% 
  initial_split(prop = 0.7)
train_df <- training(train_test_split)
test_df <- testing(train_test_split)
```

Voici ma recette. Je l‚Äôexpliquerai tout de suite apr√®s.


```{r}
recette <- recipe(yield ~ ., data = train_df) %>% 
  step_zv(all_numeric()) %>%  #retirer les variables dont la variance est non-nulle, ce qui est pratique pour √©viter que la standardisation divise par 0
  step_normalize(all_numeric(), -all_outcomes()) %>%  #je standardise avec la fonction step_normalize() et la standardisation n‚Äôest applicable que sur les entr√©es num√©riques du mod√®le
  step_downsample(topo) %>%  #retire des obsservations pour faire en sorte que les cat√©gories d‚Äôune variable apparaissent toutes en m√™me nombre
  step_dummy(topo) %>%  #je d√©sire que la variable topo subisse un enodage cat√©goriel
  prep() #Puis je m√©lange mes ingr√©dients avec cette fonction

```

```{r}
recette
```

La recette √©tant bien m√©lang√©e, on peut en extraire le jus avec la fonction `bake()`, qui permet de g√©n√©rer le tableau transform√©.

```{r}
test_proc <- bake(recette, test_df)
test_proc %>% sample_n(5)

```

La fonction `bake()` peut aussi √™tre appliqu√©e au donn√©es d‚Äôentra√Ænement, mais certaines √©tapes de recette doivent passer par des op√©rations particuli√®res, comme `step_downsample()` Il est donc pr√©f√©rable, pour les donn√©es d‚Äôentr√¢inement, d‚Äôen extraire le jus avec la fonction `juice()`.

```{r}

train_proc <- juice(recette, train_df)
train_proc %>% sample_n(5)

```

Le tableau train_proc peut √™tre envoy√© dans un mod√®le de votre choix! Par exemple,

```{r}
lm(yield ~ ., train_proc) %>% 
  summary()
```

## Analyse compositionnelle en R

En 1898, le statisticien Karl Pearson nota que des corr√©lations √©taient induites lorsque l‚Äôon effectuait des ratios par rapport √† une variable commune.

Faisons l‚Äôexercice! Nous g√©n√©rons au hasard 1000 donn√©es (comme le proposait Pearson) pour trois dimensions: le f√©mur, le tibia et l‚Äôhum√©rus. Ces dimensions ne sont pas g√©n√©r√©es par des distributions corr√©l√©es.

```{r}
set.seed(3570536)
n <- 1000
bones <- tibble(femur = rnorm(n, 10, 3),
                tibia = rnorm(n, 8, 2),
                humerus = rnorm(n, 6, 2))
plot(bones)
```



```{r}
cor(bones)
```

Pourtant, si j‚Äôutilise des ratios allom√©triques avec l‚Äôhum√©rus comme base,

```{r}
bones_r <- bones %>% 
  transmute(fh = femur/humerus,
            th = tibia/humerus)
plot(bones_r)
round(cor(bones_r$fh, bones_r$th), 2)
#text(30, 20, paste("corr√©lation =", round(cor(bones_r$fh, bones_r$th), 2)), col = "blue")

```



```{r}

```



```{r}

```



```{r}

```




```{r}

```




```{r}

```



```{r}

```




